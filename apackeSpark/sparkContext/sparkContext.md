* driver access spark functionality through sparkContext object
* represents a connection to the computing cluster
* used to build RDDs
* works with cluster manager
* manage executors running on worker nodes
* splits jobs as parallel tasks and executes them on worker node
* partition RDD and distribute them on worker node
* collects results and presents them to the driver program
